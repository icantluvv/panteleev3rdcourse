{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import BlanklineTokenizer                              \n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk.tokenize import LegalitySyllableTokenizer\n",
    "from nltk.tokenize import LineTokenizer\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk.tokenize import NLTKWordTokenizer                      \n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import SExprTokenizer\n",
    "from nltk.tokenize import SpaceTokenizer\n",
    "from nltk.tokenize import SyllableTokenizer\n",
    "from nltk.tokenize import TabTokenizer\n",
    "from nltk.tokenize import TextTilingTokenizer\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.tokenize import ReppTokenizer\n",
    "from nltk.tokenize import StanfordSegmenter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BlanklineTokenizer\n",
    "\n",
    "Как понятно из названия, разбивает текст, рассматривая в качестве разделителя любую последовательность пустых строк "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu Murakami', 'PIERCING', '1', 'A SMALL LIVING CREATURE asleep in its crib. Like a laboratory animal in a cage, thought Kawashima Masayuki. He used the palm of his hand to shade the penlight so that it illuminated only the baby’s form, leaving the rest of the bedroom in darkness. Leaning in closer, he silently mouthed the words Fast asleep. As Yoko’s pregnancy had progressed and the fact that he was actually going to be a father began to sink in, he’d worried that the baby might have difficulty sleeping. Kawashima had suffered from insomnia since elementary school, and, after all, his blood would run in this child’s veins. He’d heard it was normal for newborns to sleep virtually around the clock; in fact, he seemed to recall some child-rearing expert describing sleep as an infant’s ‘job’. What could be more tragic, then, than a baby insomniac?', 'He turned softly to check on Yoko in the double bed behind him. Her regular breathing assured him she was still asleep.', 'Kawashima had been doing this every night lately, standing there gazing down at the baby while his wife slept. Ten nights in a row now, to be exact. It was well after midnight, and since Yoko rose early each morning to prepare for work, she wasn’t likely to awaken. A wholesome and healthy twenty-nine-year-old cooking expert, Yoko was a stranger to things like insomnia. She’d quit her job with a major manufacturer of baked goods when they married and begun giving lessons to people from the neighbourhood, right here in their one-bedroom apartment. Yoko’s bread and pastry classes proved astonishingly popular, and now she had dozens of students — from housewives and middle-school girls to elderly widowers and even middle-aged men. She taught classes almost every day, taking only two fixed holidays a month, and the entire apartment, including this bedroom, was permeated with the buttery smell that for Kawashima had come to symbolise happiness. Little Rie (the name suggested by Yoko’s mother) was now four months old, and Yoko somehow managed to look after her and still maintain a full teaching schedule. Of course, it didn’t hurt that most of her students were female and always eager to help out with the baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = BlanklineTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WhitespaceTokenizer\n",
    "\n",
    "При помощи этого токенизатора исходный текст разделяется на группы символов, ограниченных пробелами (обычно, это отдельные слова, либо слова со следующими за ними знаками препинания)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage,', 'thought', 'Kawashima', 'Masayuki.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby’s', 'form,', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'Leaning', 'in', 'closer,', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep.', 'As', 'Yoko’s', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in,', 'he’d', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school,', 'and,', 'after', 'all,', 'his', 'blood', 'would', 'run', 'in', 'this', 'child’s', 'veins.', 'He’d', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock;', 'in', 'fact,', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant’s', '‘job’.', 'What', 'could', 'be', 'more', 'tragic,', 'then,', 'than', 'a', 'baby', 'insomniac?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately,', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'Ten', 'nights', 'in', 'a', 'row', 'now,', 'to', 'be', 'exact.', 'It', 'was', 'well', 'after', 'midnight,', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work,', 'she', 'wasn’t', 'likely', 'to', 'awaken.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert,', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'She’d', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood,', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'Yoko’s', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular,', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'She', 'taught', 'classes', 'almost', 'every', 'day,', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month,', 'and', 'the', 'entire', 'apartment,', 'including', 'this', 'bedroom,', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'Little', 'Rie', '(the', 'name', 'suggested', 'by', 'Yoko’s', 'mother)', 'was', 'now', 'four', 'months', 'old,', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule.', 'Of', 'course,', 'it', 'didn’t', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LegalitySyllableTokenizer\n",
    "\n",
    "Разбивает слова по слогам, основываясь на принципе правильности и максимизации наборов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ry', 'u ', 'Mu', 'ra', 'ka', 'mi\\n\\n', 'PI', 'ER', 'CING\\n\\n\\n1\\n\\n\\n', '\\nA S', 'MALL ', 'LI', 'VING C', 'RE', 'A', 'TU', 'RE', ' as', 'le', 'ep', ' in', ' its c', 'rib. ', 'Li', 'ke', ' a ', 'la', 'bo', 'ra', 'to', 'ry', ' a', 'ni', 'mal', ' in', ' a ', 'ca', 'ge, t', 'ho', 'ught ', 'Ka', 'was', 'hi', 'ma ', 'Ma', 'sa', 'y', 'u', 'ki. ', 'He', ' u', 'sed t', 'he ', 'palm', ' of ', 'his ', 'hand ', 'to s', 'ha', 'de t', 'he ', 'pen', 'light ', 'so t', 'hat', ' it', ' il', 'lu', 'mi', 'na', 'ted', ' on', 'ly t', 'he ', 'ba', 'by’s ', 'form, ', 'le', 'a', 'ving t', 'he ', 'rest', ' of t', 'he ', 'bed', 'ro', 'om', ' in ', 'dark', 'ness. ', 'Le', 'a', 'ning', ' in c', 'lo', 'ser, ', 'he ', 'si', 'lent', 'ly ', 'mo', 'ut', 'hed t', 'he ', 'words ', 'Fast', ' as', 'le', 'ep.', ' As', ' Y', 'o', 'ko’s p', 'reg', 'nan', 'cy ', 'had p', 'rog', 'res', 'sed', ' and t', 'he ', 'fact t', 'hat ', 'he ', 'was', ' ac', 'tu', 'al', 'ly ', 'go', 'ing ', 'to ', 'be', ' a ', 'fat', 'her ', 'be', 'gan ', 'to ', 'sink', ' in, ', 'he’d ', 'wor', 'ri', 'ed t', 'hat t', 'he ', 'ba', 'by ', 'might ', 'ha', 've ', 'dif', 'fi', 'cul', 'ty s', 'le', 'e', 'ping. ', 'Ka', 'was', 'hi', 'ma ', 'had ', 'suf', 'fe', 'red f', 'rom', ' in', 'som', 'ni', 'a ', 'sin', 'ce', ' e', 'le', 'men', 'ta', 'ry sc', 'ho', 'ol,', ' and,', ' af', 'ter', ' all, ', 'his b', 'lo', 'od ', 'wo', 'uld ', 'run', ' in t', 'his c', 'hild’s ', 've', 'ins. ', 'He’d ', 'he', 'ard', ' it ', 'was ', 'nor', 'mal ', 'for ', 'new', 'borns ', 'to s', 'le', 'ep ', 'vir', 'tu', 'al', 'ly', ' a', 'ro', 'und t', 'he c', 'lock;', ' in ', 'fact, ', 'he ', 'se', 'e', 'med ', 'to ', 're', 'call ', 'so', 'me c', 'hild-', 're', 'a', 'ring', ' ex', 'pert ', 'desc', 'ri', 'bing s', 'le', 'ep', ' as', ' an', ' in', 'fant’s ‘', 'job’. W', 'hat ', 'co', 'uld ', 'be ', 'mo', 're t', 'ra', 'gic, t', 'hen, t', 'han', ' a ', 'ba', 'by', ' in', 'som', 'ni', 'ac?\\n\\n', 'He ', 'tur', 'ned ', 'soft', 'ly ', 'to c', 'heck', ' on', ' Y', 'o', 'ko', ' in t', 'he ', 'do', 'ub', 'le ', 'bed ', 'be', 'hind ', 'him. ', 'Her ', 're', 'gu', 'lar b', 're', 'at', 'hing', ' as', 'su', 'red ', 'him s', 'he ', 'was s', 'till', ' as', 'le', 'ep.\\n\\n', 'Ka', 'was', 'hi', 'ma ', 'had ', 'be', 'en ', 'do', 'ing t', 'his', ' e', 've', 'ry ', 'night ', 'la', 'te', 'ly, s', 'tan', 'ding t', 'he', 're ', 'gaz', 'ing ', 'down', ' at t', 'he ', 'ba', 'by w', 'hi', 'le ', 'his ', 'wi', 'fe s', 'lept. ', 'Ten ', 'nights', ' in', ' a ', 'row ', 'now, ', 'to ', 'be', ' e', 'xact.', ' It ', 'was ', 'well', ' af', 'ter ', 'mid', 'night,', ' and ', 'sin', 'ce', ' Y', 'o', 'ko ', 'ro', 'se', ' e', 'ar', 'ly', ' e', 'ach ', 'mor', 'ning ', 'to p', 're', 'pa', 're ', 'for ', 'work, s', 'he ', 'wasn’t ', 'li', 'ke', 'ly ', 'to', ' a', 'wa', 'ken.', ' A w', 'ho', 'le', 'so', 'me', ' and ', 'he', 'alt', 'hy t', 'wen', 'ty-', 'ni', 'ne', '-y', 'e', 'ar', '-old ', 'co', 'o', 'king', ' ex', 'pert,', ' Y', 'o', 'ko ', 'was', ' a st', 'ran', 'ger ', 'to t', 'hings ', 'li', 'ke', ' in', 'som', 'ni', 'a. S', 'he’d q', 'u', 'it ', 'her ', 'job ', 'with', ' a ', 'ma', 'jor ', 'ma', 'nu', 'fac', 'tu', 'rer', ' of ', 'ba', 'ked ', 'go', 'ods w', 'hen t', 'he', 'y ', 'mar', 'ri', 'ed', ' and ', 'be', 'gun ', 'gi', 'ving ', 'les', 'sons ', 'to ', 'pe', 'op', 'le f', 'rom t', 'he ', 'ne', 'igh', 'bo', 'ur', 'ho', 'od, ', 'right ', 'he', 're', ' in t', 'he', 'ir', ' o', 'ne-', 'bed', 'ro', 'om', ' a', 'part', 'ment.', ' Y', 'o', 'ko’s b', 're', 'ad', ' and ', 'past', 'ry c', 'las', 'ses p', 'ro', 'ved', ' as', 'to', 'nis', 'hing', 'ly ', 'po', 'pu', 'lar,', ' and ', 'now s', 'he ', 'had ', 'doz', 'ens', ' of s', 'tu', 'dents — f', 'rom ', 'ho', 'u', 'se', 'wi', 'ves', ' and ', 'midd', 'le-sc', 'ho', 'ol ', 'girls ', 'to', ' el', 'der', 'ly ', 'wi', 'do', 'wers', ' and', ' e', 'ven ', 'midd', 'le', '-a', 'ged ', 'men. S', 'he ', 'ta', 'ught c', 'las', 'ses', ' al', 'most', ' e', 've', 'ry ', 'da', 'y, ', 'ta', 'king', ' on', 'ly t', 'wo ', 'fi', 'xed ', 'ho', 'li', 'da', 'ys', ' a ', 'month,', ' and t', 'he', ' en', 'ti', 're', ' a', 'part', 'ment,', ' inc', 'lu', 'ding t', 'his ', 'bed', 'ro', 'om, ', 'was ', 'per', 'me', 'a', 'ted ', 'with t', 'he ', 'but', 'te', 'ry s', 'mell t', 'hat ', 'for ', 'Ka', 'was', 'hi', 'ma ', 'had ', 'co', 'me ', 'to ', 'sym', 'bo', 'li', 'se ', 'hap', 'pi', 'ness. ', 'Litt', 'le ', 'Ri', 'e (t', 'he ', 'na', 'me ', 'sug', 'ges', 'ted ', 'by', ' Y', 'o', 'ko’s ', 'mot', 'her) ', 'was ', 'now ', 'fo', 'ur ', 'months', ' old,', ' and', ' Y', 'o', 'ko ', 'so', 'me', 'how ', 'ma', 'na', 'ged ', 'to ', 'lo', 'ok', ' af', 'ter ', 'her', ' and s', 'till ', 'ma', 'in', 'ta', 'in', ' a ', 'full ', 'te', 'ac', 'hing sc', 'he', 'du', 'le.', ' Of ', 'co', 'ur', 'se,', ' it ', 'didn’t ', 'hurt t', 'hat ', 'most', ' of ', 'her s', 'tu', 'dents ', 'we', 're ', 'fe', 'ma', 'le', ' and', ' al', 'wa', 'ys', ' e', 'a', 'ger ', 'to ', 'help', ' o', 'ut ', 'with t', 'he ', 'ba', 'by.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = LegalitySyllableTokenizer(tokenized_source_text = text)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LineTokenizer\n",
    "\n",
    "Аналогичен SpaceTokenizer, за исключением того, что для разделения строки использует символ новой строки (при необходимости может отбрасывать пустые строки). Его применение похоже на применение метода s.split(‘\\n’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu Murakami', 'PIERCING', '1', 'A SMALL LIVING CREATURE asleep in its crib. Like a laboratory animal in a cage, thought Kawashima Masayuki. He used the palm of his hand to shade the penlight so that it illuminated only the baby’s form, leaving the rest of the bedroom in darkness. Leaning in closer, he silently mouthed the words Fast asleep. As Yoko’s pregnancy had progressed and the fact that he was actually going to be a father began to sink in, he’d worried that the baby might have difficulty sleeping. Kawashima had suffered from insomnia since elementary school, and, after all, his blood would run in this child’s veins. He’d heard it was normal for newborns to sleep virtually around the clock; in fact, he seemed to recall some child-rearing expert describing sleep as an infant’s ‘job’. What could be more tragic, then, than a baby insomniac?', 'He turned softly to check on Yoko in the double bed behind him. Her regular breathing assured him she was still asleep.', 'Kawashima had been doing this every night lately, standing there gazing down at the baby while his wife slept. Ten nights in a row now, to be exact. It was well after midnight, and since Yoko rose early each morning to prepare for work, she wasn’t likely to awaken. A wholesome and healthy twenty-nine-year-old cooking expert, Yoko was a stranger to things like insomnia. She’d quit her job with a major manufacturer of baked goods when they married and begun giving lessons to people from the neighbourhood, right here in their one-bedroom apartment. Yoko’s bread and pastry classes proved astonishingly popular, and now she had dozens of students — from housewives and middle-school girls to elderly widowers and even middle-aged men. She taught classes almost every day, taking only two fixed holidays a month, and the entire apartment, including this bedroom, was permeated with the buttery smell that for Kawashima had come to symbolise happiness. Little Rie (the name suggested by Yoko’s mother) was now four months old, and Yoko somehow managed to look after her and still maintain a full teaching schedule. Of course, it didn’t hurt that most of her students were female and always eager to help out with the baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = LineTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MWETokenizer (Multi-Word Expression Tokenizer)\n",
    "\n",
    "MWETokenizer берет строку, которая уже была разделена на токены, и повторно токенизирует ее, объединяя выражения из нескольких слов в отдельные токены, используя словарь MWE. Э"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['R', 'y', 'u', ' ', 'M', 'u', 'r', 'a', 'k', 'a', 'm', 'i', '\\n', '\\n', 'P', 'I', 'E', 'R', 'C', 'I', 'N', 'G', '\\n', '\\n', '\\n', '1', '\\n', '\\n', '\\n', '\\n', 'A', ' ', 'S', 'M', 'A', 'L', 'L', ' ', 'L', 'I', 'V', 'I', 'N', 'G', ' ', 'C', 'R', 'E', 'A', 'T', 'U', 'R', 'E', ' ', 'a', 's', 'l', 'e', 'e', 'p', ' ', 'i', 'n', ' ', 'i', 't', 's', ' ', 'c', 'r', 'i', 'b', '.', ' ', 'L', 'i', 'k', 'e', ' ', 'a', ' ', 'l', 'a', 'b', 'o', 'r', 'a', 't', 'o', 'r', 'y', ' ', 'a', 'n', 'i', 'm', 'a', 'l', ' ', 'i', 'n', ' ', 'a', ' ', 'c', 'a', 'g', 'e', ',', ' ', 't', 'h', 'o', 'u', 'g', 'h', 't', ' ', 'K', 'a', 'w', 'a', 's', 'h', 'i', 'm', 'a', ' ', 'M', 'a', 's', 'a', 'y', 'u', 'k', 'i', '.', ' ', 'H', 'e', ' ', 'u', 's', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'p', 'a', 'l', 'm', ' ', 'o', 'f', ' ', 'h', 'i', 's', ' ', 'h', 'a', 'n', 'd', ' ', 't', 'o', ' ', 's', 'h', 'a', 'd', 'e', ' ', 't', 'h', 'e', ' ', 'p', 'e', 'n', 'l', 'i', 'g', 'h', 't', ' ', 's', 'o', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'i', 'l', 'l', 'u', 'm', 'i', 'n', 'a', 't', 'e', 'd', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'b', 'y', '’', 's', ' ', 'f', 'o', 'r', 'm', ',', ' ', 'l', 'e', 'a', 'v', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'r', 'e', 's', 't', ' ', 'o', 'f', ' ', 't', 'h', 'e', ' ', 'b', 'e', 'd', 'r', 'o', 'o', 'm', ' ', 'i', 'n', ' ', 'd', 'a', 'r', 'k', 'n', 'e', 's', 's', '.', ' ', 'L', 'e', 'a', 'n', 'i', 'n', 'g', ' ', 'i', 'n', ' ', 'c', 'l', 'o', 's', 'e', 'r', ',', ' ', 'h', 'e', ' ', 's', 'i', 'l', 'e', 'n', 't', 'l', 'y', ' ', 'm', 'o', 'u', 't', 'h', 'e', 'd', ' ', 't', 'h', 'e', ' ', 'w', 'o', 'r', 'd', 's', ' ', 'F', 'a', 's', 't', ' ', 'a', 's', 'l', 'e', 'e', 'p', '.', ' ', 'A', 's', ' ', 'Y', 'o', 'k', 'o', '’', 's', ' ', 'p', 'r', 'e', 'g', 'n', 'a', 'n', 'c', 'y', ' ', 'h', 'a', 'd', ' ', 'p', 'r', 'o', 'g', 'r', 'e', 's', 's', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'f', 'a', 'c', 't', ' ', 't', 'h', 'a', 't', ' ', 'h', 'e', ' ', 'w', 'a', 's', ' ', 'a', 'c', 't', 'u', 'a', 'l', 'l', 'y', ' ', 'g', 'o', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'a', ' ', 'f', 'a', 't', 'h', 'e', 'r', ' ', 'b', 'e', 'g', 'a', 'n', ' ', 't', 'o', ' ', 's', 'i', 'n', 'k', ' ', 'i', 'n', ',', ' ', 'h', 'e', '’', 'd', ' ', 'w', 'o', 'r', 'r', 'i', 'e', 'd', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'b', 'y', ' ', 'm', 'i', 'g', 'h', 't', ' ', 'h', 'a', 'v', 'e', ' ', 'd', 'i', 'f', 'f', 'i', 'c', 'u', 'l', 't', 'y', ' ', 's', 'l', 'e', 'e', 'p', 'i', 'n', 'g', '.', ' ', 'K', 'a', 'w', 'a', 's', 'h', 'i', 'm', 'a', ' ', 'h', 'a', 'd', ' ', 's', 'u', 'f', 'f', 'e', 'r', 'e', 'd', ' ', 'f', 'r', 'o', 'm', ' ', 'i', 'n', 's', 'o', 'm', 'n', 'i', 'a', ' ', 's', 'i', 'n', 'c', 'e', ' ', 'e', 'l', 'e', 'm', 'e', 'n', 't', 'a', 'r', 'y', ' ', 's', 'c', 'h', 'o', 'o', 'l', ',', ' ', 'a', 'n', 'd', ',', ' ', 'a', 'f', 't', 'e', 'r', ' ', 'a', 'l', 'l', ',', ' ', 'h', 'i', 's', ' ', 'b', 'l', 'o', 'o', 'd', ' ', 'w', 'o', 'u', 'l', 'd', ' ', 'r', 'u', 'n', ' ', 'i', 'n', ' ', 't', 'h', 'i', 's', ' ', 'c', 'h', 'i', 'l', 'd', '’', 's', ' ', 'v', 'e', 'i', 'n', 's', '.', ' ', 'H', 'e', '’', 'd', ' ', 'h', 'e', 'a', 'r', 'd', ' ', 'i', 't', ' ', 'w', 'a', 's', ' ', 'n', 'o', 'r', 'm', 'a', 'l', ' ', 'f', 'o', 'r', ' ', 'n', 'e', 'w', 'b', 'o', 'r', 'n', 's', ' ', 't', 'o', ' ', 's', 'l', 'e', 'e', 'p', ' ', 'v', 'i', 'r', 't', 'u', 'a', 'l', 'l', 'y', ' ', 'a', 'r', 'o', 'u', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'c', 'l', 'o', 'c', 'k', ';', ' ', 'i', 'n', ' ', 'f', 'a', 'c', 't', ',', ' ', 'h', 'e', ' ', 's', 'e', 'e', 'm', 'e', 'd', ' ', 't', 'o', ' ', 'r', 'e', 'c', 'a', 'l', 'l', ' ', 's', 'o', 'm', 'e', ' ', 'c', 'h', 'i', 'l', 'd', '-', 'r', 'e', 'a', 'r', 'i', 'n', 'g', ' ', 'e', 'x', 'p', 'e', 'r', 't', ' ', 'd', 'e', 's', 'c', 'r', 'i', 'b', 'i', 'n', 'g', ' ', 's', 'l', 'e', 'e', 'p', ' ', 'a', 's', ' ', 'a', 'n', ' ', 'i', 'n', 'f', 'a', 'n', 't', '’', 's', ' ', '‘', 'j', 'o', 'b', '’', '.', ' ', 'W', 'h', 'a', 't', ' ', 'c', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'm', 'o', 'r', 'e', ' ', 't', 'r', 'a', 'g', 'i', 'c', ',', ' ', 't', 'h', 'e', 'n', ',', ' ', 't', 'h', 'a', 'n', ' ', 'a', ' ', 'b', 'a', 'b', 'y', ' ', 'i', 'n', 's', 'o', 'm', 'n', 'i', 'a', 'c', '?', '\\n', '\\n', 'H', 'e', ' ', 't', 'u', 'r', 'n', 'e', 'd', ' ', 's', 'o', 'f', 't', 'l', 'y', ' ', 't', 'o', ' ', 'c', 'h', 'e', 'c', 'k', ' ', 'o', 'n', ' ', 'Y', 'o', 'k', 'o', ' ', 'i', 'n', ' ', 't', 'h', 'e', ' ', 'd', 'o', 'u', 'b', 'l', 'e', ' ', 'b', 'e', 'd', ' ', 'b', 'e', 'h', 'i', 'n', 'd', ' ', 'h', 'i', 'm', '.', ' ', 'H', 'e', 'r', ' ', 'r', 'e', 'g', 'u', 'l', 'a', 'r', ' ', 'b', 'r', 'e', 'a', 't', 'h', 'i', 'n', 'g', ' ', 'a', 's', 's', 'u', 'r', 'e', 'd', ' ', 'h', 'i', 'm', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', ' ', 's', 't', 'i', 'l', 'l', ' ', 'a', 's', 'l', 'e', 'e', 'p', '.', '\\n', '\\n', 'K', 'a', 'w', 'a', 's', 'h', 'i', 'm', 'a', ' ', 'h', 'a', 'd', ' ', 'b', 'e', 'e', 'n', ' ', 'd', 'o', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'e', 'v', 'e', 'r', 'y', ' ', 'n', 'i', 'g', 'h', 't', ' ', 'l', 'a', 't', 'e', 'l', 'y', ',', ' ', 's', 't', 'a', 'n', 'd', 'i', 'n', 'g', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'g', 'a', 'z', 'i', 'n', 'g', ' ', 'd', 'o', 'w', 'n', ' ', 'a', 't', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'b', 'y', ' ', 'w', 'h', 'i', 'l', 'e', ' ', 'h', 'i', 's', ' ', 'w', 'i', 'f', 'e', ' ', 's', 'l', 'e', 'p', 't', '.', ' ', 'T', 'e', 'n', ' ', 'n', 'i', 'g', 'h', 't', 's', ' ', 'i', 'n', ' ', 'a', ' ', 'r', 'o', 'w', ' ', 'n', 'o', 'w', ',', ' ', 't', 'o', ' ', 'b', 'e', ' ', 'e', 'x', 'a', 'c', 't', '.', ' ', 'I', 't', ' ', 'w', 'a', 's', ' ', 'w', 'e', 'l', 'l', ' ', 'a', 'f', 't', 'e', 'r', ' ', 'm', 'i', 'd', 'n', 'i', 'g', 'h', 't', ',', ' ', 'a', 'n', 'd', ' ', 's', 'i', 'n', 'c', 'e', ' ', 'Y', 'o', 'k', 'o', ' ', 'r', 'o', 's', 'e', ' ', 'e', 'a', 'r', 'l', 'y', ' ', 'e', 'a', 'c', 'h', ' ', 'm', 'o', 'r', 'n', 'i', 'n', 'g', ' ', 't', 'o', ' ', 'p', 'r', 'e', 'p', 'a', 'r', 'e', ' ', 'f', 'o', 'r', ' ', 'w', 'o', 'r', 'k', ',', ' ', 's', 'h', 'e', ' ', 'w', 'a', 's', 'n', '’', 't', ' ', 'l', 'i', 'k', 'e', 'l', 'y', ' ', 't', 'o', ' ', 'a', 'w', 'a', 'k', 'e', 'n', '.', ' ', 'A', ' ', 'w', 'h', 'o', 'l', 'e', 's', 'o', 'm', 'e', ' ', 'a', 'n', 'd', ' ', 'h', 'e', 'a', 'l', 't', 'h', 'y', ' ', 't', 'w', 'e', 'n', 't', 'y', '-', 'n', 'i', 'n', 'e', '-', 'y', 'e', 'a', 'r', '-', 'o', 'l', 'd', ' ', 'c', 'o', 'o', 'k', 'i', 'n', 'g', ' ', 'e', 'x', 'p', 'e', 'r', 't', ',', ' ', 'Y', 'o', 'k', 'o', ' ', 'w', 'a', 's', ' ', 'a', ' ', 's', 't', 'r', 'a', 'n', 'g', 'e', 'r', ' ', 't', 'o', ' ', 't', 'h', 'i', 'n', 'g', 's', ' ', 'l', 'i', 'k', 'e', ' ', 'i', 'n', 's', 'o', 'm', 'n', 'i', 'a', '.', ' ', 'S', 'h', 'e', '’', 'd', ' ', 'q', 'u', 'i', 't', ' ', 'h', 'e', 'r', ' ', 'j', 'o', 'b', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'm', 'a', 'j', 'o', 'r', ' ', 'm', 'a', 'n', 'u', 'f', 'a', 'c', 't', 'u', 'r', 'e', 'r', ' ', 'o', 'f', ' ', 'b', 'a', 'k', 'e', 'd', ' ', 'g', 'o', 'o', 'd', 's', ' ', 'w', 'h', 'e', 'n', ' ', 't', 'h', 'e', 'y', ' ', 'm', 'a', 'r', 'r', 'i', 'e', 'd', ' ', 'a', 'n', 'd', ' ', 'b', 'e', 'g', 'u', 'n', ' ', 'g', 'i', 'v', 'i', 'n', 'g', ' ', 'l', 'e', 's', 's', 'o', 'n', 's', ' ', 't', 'o', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'f', 'r', 'o', 'm', ' ', 't', 'h', 'e', ' ', 'n', 'e', 'i', 'g', 'h', 'b', 'o', 'u', 'r', 'h', 'o', 'o', 'd', ',', ' ', 'r', 'i', 'g', 'h', 't', ' ', 'h', 'e', 'r', 'e', ' ', 'i', 'n', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'o', 'n', 'e', '-', 'b', 'e', 'd', 'r', 'o', 'o', 'm', ' ', 'a', 'p', 'a', 'r', 't', 'm', 'e', 'n', 't', '.', ' ', 'Y', 'o', 'k', 'o', '’', 's', ' ', 'b', 'r', 'e', 'a', 'd', ' ', 'a', 'n', 'd', ' ', 'p', 'a', 's', 't', 'r', 'y', ' ', 'c', 'l', 'a', 's', 's', 'e', 's', ' ', 'p', 'r', 'o', 'v', 'e', 'd', ' ', 'a', 's', 't', 'o', 'n', 'i', 's', 'h', 'i', 'n', 'g', 'l', 'y', ' ', 'p', 'o', 'p', 'u', 'l', 'a', 'r', ',', ' ', 'a', 'n', 'd', ' ', 'n', 'o', 'w', ' ', 's', 'h', 'e', ' ', 'h', 'a', 'd', ' ', 'd', 'o', 'z', 'e', 'n', 's', ' ', 'o', 'f', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', 's', ' ', '—', ' ', 'f', 'r', 'o', 'm', ' ', 'h', 'o', 'u', 's', 'e', 'w', 'i', 'v', 'e', 's', ' ', 'a', 'n', 'd', ' ', 'm', 'i', 'd', 'd', 'l', 'e', '-', 's', 'c', 'h', 'o', 'o', 'l', ' ', 'g', 'i', 'r', 'l', 's', ' ', 't', 'o', ' ', 'e', 'l', 'd', 'e', 'r', 'l', 'y', ' ', 'w', 'i', 'd', 'o', 'w', 'e', 'r', 's', ' ', 'a', 'n', 'd', ' ', 'e', 'v', 'e', 'n', ' ', 'm', 'i', 'd', 'd', 'l', 'e', '-', 'a', 'g', 'e', 'd', ' ', 'm', 'e', 'n', '.', ' ', 'S', 'h', 'e', ' ', 't', 'a', 'u', 'g', 'h', 't', ' ', 'c', 'l', 'a', 's', 's', 'e', 's', ' ', 'a', 'l', 'm', 'o', 's', 't', ' ', 'e', 'v', 'e', 'r', 'y', ' ', 'd', 'a', 'y', ',', ' ', 't', 'a', 'k', 'i', 'n', 'g', ' ', 'o', 'n', 'l', 'y', ' ', 't', 'w', 'o', ' ', 'f', 'i', 'x', 'e', 'd', ' ', 'h', 'o', 'l', 'i', 'd', 'a', 'y', 's', ' ', 'a', ' ', 'm', 'o', 'n', 't', 'h', ',', ' ', 'a', 'n', 'd', ' ', 't', 'h', 'e', ' ', 'e', 'n', 't', 'i', 'r', 'e', ' ', 'a', 'p', 'a', 'r', 't', 'm', 'e', 'n', 't', ',', ' ', 'i', 'n', 'c', 'l', 'u', 'd', 'i', 'n', 'g', ' ', 't', 'h', 'i', 's', ' ', 'b', 'e', 'd', 'r', 'o', 'o', 'm', ',', ' ', 'w', 'a', 's', ' ', 'p', 'e', 'r', 'm', 'e', 'a', 't', 'e', 'd', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'b', 'u', 't', 't', 'e', 'r', 'y', ' ', 's', 'm', 'e', 'l', 'l', ' ', 't', 'h', 'a', 't', ' ', 'f', 'o', 'r', ' ', 'K', 'a', 'w', 'a', 's', 'h', 'i', 'm', 'a', ' ', 'h', 'a', 'd', ' ', 'c', 'o', 'm', 'e', ' ', 't', 'o', ' ', 's', 'y', 'm', 'b', 'o', 'l', 'i', 's', 'e', ' ', 'h', 'a', 'p', 'p', 'i', 'n', 'e', 's', 's', '.', ' ', 'L', 'i', 't', 't', 'l', 'e', ' ', 'R', 'i', 'e', ' ', '(', 't', 'h', 'e', ' ', 'n', 'a', 'm', 'e', ' ', 's', 'u', 'g', 'g', 'e', 's', 't', 'e', 'd', ' ', 'b', 'y', ' ', 'Y', 'o', 'k', 'o', '’', 's', ' ', 'm', 'o', 't', 'h', 'e', 'r', ')', ' ', 'w', 'a', 's', ' ', 'n', 'o', 'w', ' ', 'f', 'o', 'u', 'r', ' ', 'm', 'o', 'n', 't', 'h', 's', ' ', 'o', 'l', 'd', ',', ' ', 'a', 'n', 'd', ' ', 'Y', 'o', 'k', 'o', ' ', 's', 'o', 'm', 'e', 'h', 'o', 'w', ' ', 'm', 'a', 'n', 'a', 'g', 'e', 'd', ' ', 't', 'o', ' ', 'l', 'o', 'o', 'k', ' ', 'a', 'f', 't', 'e', 'r', ' ', 'h', 'e', 'r', ' ', 'a', 'n', 'd', ' ', 's', 't', 'i', 'l', 'l', ' ', 'm', 'a', 'i', 'n', 't', 'a', 'i', 'n', ' ', 'a', ' ', 'f', 'u', 'l', 'l', ' ', 't', 'e', 'a', 'c', 'h', 'i', 'n', 'g', ' ', 's', 'c', 'h', 'e', 'd', 'u', 'l', 'e', '.', ' ', 'O', 'f', ' ', 'c', 'o', 'u', 'r', 's', 'e', ',', ' ', 'i', 't', ' ', 'd', 'i', 'd', 'n', '’', 't', ' ', 'h', 'u', 'r', 't', ' ', 't', 'h', 'a', 't', ' ', 'm', 'o', 's', 't', ' ', 'o', 'f', ' ', 'h', 'e', 'r', ' ', 's', 't', 'u', 'd', 'e', 'n', 't', 's', ' ', 'w', 'e', 'r', 'e', ' ', 'f', 'e', 'm', 'a', 'l', 'e', ' ', 'a', 'n', 'd', ' ', 'a', 'l', 'w', 'a', 'y', 's', ' ', 'e', 'a', 'g', 'e', 'r', ' ', 't', 'o', ' ', 'h', 'e', 'l', 'p', ' ', 'o', 'u', 't', ' ', 'w', 'i', 't', 'h', ' ', 't', 'h', 'e', ' ', 'b', 'a', 'b', 'y', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = MWETokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTKWordTokenizer\n",
    "\n",
    "Это улучшенный вариант TreebankWordTokenizer, включающий несколько адаптированных списков сокращений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', ',', 'thought', 'Kawashima', 'Masayuki.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby', '’', 's', 'form', ',', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'Leaning', 'in', 'closer', ',', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep.', 'As', 'Yoko', '’', 's', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', ',', 'he', '’', 'd', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', ',', 'and', ',', 'after', 'all', ',', 'his', 'blood', 'would', 'run', 'in', 'this', 'child', '’', 's', 'veins.', 'He', '’', 'd', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock', ';', 'in', 'fact', ',', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant', '’', 's', '‘', 'job', '’', '.', 'What', 'could', 'be', 'more', 'tragic', ',', 'then', ',', 'than', 'a', 'baby', 'insomniac', '?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', ',', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'Ten', 'nights', 'in', 'a', 'row', 'now', ',', 'to', 'be', 'exact.', 'It', 'was', 'well', 'after', 'midnight', ',', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', ',', 'she', 'wasn', '’', 't', 'likely', 'to', 'awaken.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert', ',', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'She', '’', 'd', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', ',', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'Yoko', '’', 's', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', ',', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'She', 'taught', 'classes', 'almost', 'every', 'day', ',', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', ',', 'and', 'the', 'entire', 'apartment', ',', 'including', 'this', 'bedroom', ',', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'Little', 'Rie', '(', 'the', 'name', 'suggested', 'by', 'Yoko', '’', 's', 'mother', ')', 'was', 'now', 'four', 'months', 'old', ',', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule.', 'Of', 'course', ',', 'it', 'didn', '’', 't', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = NLTKWordTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PunktSentenceTokenizer\n",
    "\n",
    "Этот токенизатор разделяет текст на список предложений, используя алгоритм обучения без учителя (Unsupervised learning) для построения модели слов-сокращений, словосочетаний и слов, с которых начинаются предложения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu Murakami\\n\\nPIERCING\\n\\n\\n1\\n\\n\\n\\nA SMALL LIVING CREATURE asleep in its crib.', 'Like a laboratory animal in a cage, thought Kawashima Masayuki.', 'He used the palm of his hand to shade the penlight so that it illuminated only the baby’s form, leaving the rest of the bedroom in darkness.', 'Leaning in closer, he silently mouthed the words Fast asleep.', 'As Yoko’s pregnancy had progressed and the fact that he was actually going to be a father began to sink in, he’d worried that the baby might have difficulty sleeping.', 'Kawashima had suffered from insomnia since elementary school, and, after all, his blood would run in this child’s veins.', 'He’d heard it was normal for newborns to sleep virtually around the clock; in fact, he seemed to recall some child-rearing expert describing sleep as an infant’s ‘job’.', 'What could be more tragic, then, than a baby insomniac?', 'He turned softly to check on Yoko in the double bed behind him.', 'Her regular breathing assured him she was still asleep.', 'Kawashima had been doing this every night lately, standing there gazing down at the baby while his wife slept.', 'Ten nights in a row now, to be exact.', 'It was well after midnight, and since Yoko rose early each morning to prepare for work, she wasn’t likely to awaken.', 'A wholesome and healthy twenty-nine-year-old cooking expert, Yoko was a stranger to things like insomnia.', 'She’d quit her job with a major manufacturer of baked goods when they married and begun giving lessons to people from the neighbourhood, right here in their one-bedroom apartment.', 'Yoko’s bread and pastry classes proved astonishingly popular, and now she had dozens of students — from housewives and middle-school girls to elderly widowers and even middle-aged men.', 'She taught classes almost every day, taking only two fixed holidays a month, and the entire apartment, including this bedroom, was permeated with the buttery smell that for Kawashima had come to symbolise happiness.', 'Little Rie (the name suggested by Yoko’s mother) was now four months old, and Yoko somehow managed to look after her and still maintain a full teaching schedule.', 'Of course, it didn’t hurt that most of her students were female and always eager to help out with the baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = PunktSentenceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RegexpTokenizer\n",
    "\n",
    "Этот токенизатор разбивает исходный текст на подстроки, используя регулярное выражение, передаваемое ему в качестве параметра. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', 'thought', 'Kawashima', 'Masayuki', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby', 's', 'form', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness', 'Leaning', 'in', 'closer', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep', 'As', 'Yoko', 's', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', 'he', 'd', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', 'and', 'after', 'all', 'his', 'blood', 'would', 'run', 'in', 'this', 'child', 's', 'veins', 'He', 'd', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock', 'in', 'fact', 'he', 'seemed', 'to', 'recall', 'some', 'child', 'rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant', 's', 'job', 'What', 'could', 'be', 'more', 'tragic', 'then', 'than', 'a', 'baby', 'insomniac', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept', 'Ten', 'nights', 'in', 'a', 'row', 'now', 'to', 'be', 'exact', 'It', 'was', 'well', 'after', 'midnight', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', 'she', 'wasn', 't', 'likely', 'to', 'awaken', 'A', 'wholesome', 'and', 'healthy', 'twenty', 'nine', 'year', 'old', 'cooking', 'expert', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia', 'She', 'd', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', 'right', 'here', 'in', 'their', 'one', 'bedroom', 'apartment', 'Yoko', 's', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', 'from', 'housewives', 'and', 'middle', 'school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle', 'aged', 'men', 'She', 'taught', 'classes', 'almost', 'every', 'day', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', 'and', 'the', 'entire', 'apartment', 'including', 'this', 'bedroom', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness', 'Little', 'Rie', 'the', 'name', 'suggested', 'by', 'Yoko', 's', 'mother', 'was', 'now', 'four', 'months', 'old', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule', 'Of', 'course', 'it', 'didn', 't', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SExprTokenizer\n",
    "\n",
    "Этот токенизатор используется для поиска в исходном тексте выражений в скобках. В частности, он делит строку на последовательность подстрок, являющихся выражениями в скобках (включая любые вложенные выражения в скобках) или другими токенами, разделенными пробелами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage,', 'thought', 'Kawashima', 'Masayuki.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby’s', 'form,', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'Leaning', 'in', 'closer,', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep.', 'As', 'Yoko’s', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in,', 'he’d', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school,', 'and,', 'after', 'all,', 'his', 'blood', 'would', 'run', 'in', 'this', 'child’s', 'veins.', 'He’d', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock;', 'in', 'fact,', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant’s', '‘job’.', 'What', 'could', 'be', 'more', 'tragic,', 'then,', 'than', 'a', 'baby', 'insomniac?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately,', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'Ten', 'nights', 'in', 'a', 'row', 'now,', 'to', 'be', 'exact.', 'It', 'was', 'well', 'after', 'midnight,', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work,', 'she', 'wasn’t', 'likely', 'to', 'awaken.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert,', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'She’d', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood,', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'Yoko’s', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular,', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'She', 'taught', 'classes', 'almost', 'every', 'day,', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month,', 'and', 'the', 'entire', 'apartment,', 'including', 'this', 'bedroom,', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'Little', 'Rie', '(the name suggested by Yoko’s mother)', ' was now four months old, and Yoko somehow managed to look after her and still maintain a full teaching schedule. Of course, it didn’t hurt that most of her students were female and always eager to help out with the baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = SExprTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SpaceTokenizer\n",
    "\n",
    "Разбивает строку на токены, используя в качестве разделителя пробел. Результат работы этого метода полностью совпадает с результатом метода s.split(‘ ‘)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami\\n\\nPIERCING\\n\\n\\n1\\n\\n\\n\\nA', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage,', 'thought', 'Kawashima', 'Masayuki.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby’s', 'form,', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'Leaning', 'in', 'closer,', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep.', 'As', 'Yoko’s', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in,', 'he’d', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school,', 'and,', 'after', 'all,', 'his', 'blood', 'would', 'run', 'in', 'this', 'child’s', 'veins.', 'He’d', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock;', 'in', 'fact,', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant’s', '‘job’.', 'What', 'could', 'be', 'more', 'tragic,', 'then,', 'than', 'a', 'baby', 'insomniac?\\n\\nHe', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.\\n\\nKawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately,', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'Ten', 'nights', 'in', 'a', 'row', 'now,', 'to', 'be', 'exact.', 'It', 'was', 'well', 'after', 'midnight,', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work,', 'she', 'wasn’t', 'likely', 'to', 'awaken.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert,', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'She’d', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood,', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'Yoko’s', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular,', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'She', 'taught', 'classes', 'almost', 'every', 'day,', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month,', 'and', 'the', 'entire', 'apartment,', 'including', 'this', 'bedroom,', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'Little', 'Rie', '(the', 'name', 'suggested', 'by', 'Yoko’s', 'mother)', 'was', 'now', 'four', 'months', 'old,', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule.', 'Of', 'course,', 'it', 'didn’t', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\")as file:\n",
    "    text = file.read()\n",
    "tokenizer = SpaceTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SyllableTokenizer\n",
    "\n",
    "Это еще один токенизатор для разбиения слов на слоги. Его работа основана на принципе последовательности звучания (Sonority Sequencing Principle, SSP). Алгоритм SSP так же, как и алгоритм Legality Principle не зависит от используемого языка. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', ' ', 'Mu', 'ra', 'ka', 'mi\\n', '\\nPIERCIN', 'G\\n\\n', '\\n1', '\\n\\n', '\\n', '\\nA', ' SMAL', 'L LIVIN', 'G CREATU', 'RE ', 'a', 'slee', 'p i', 'n i', 'ts ', 'crib', '.', ' ', 'Li', 'ke ', 'a', ' ', 'la', 'bo', 'ra', 'to', 'ry ', 'a', 'ni', 'ma', 'l i', 'n a', ' ', 'ca', 'ge', ',', ' t', 'hough', 't ', 'Ka', 'was', 'hi', 'ma ', 'Ma', 'say', 'u', 'ki', '.', ' ', 'He ', 'u', 'se', 'd t', 'he ', 'pal', 'm o', 'f ', 'hi', 's ', 'han', 'd ', 'to s', 'ha', 'de t', 'he ', 'pen', 'ligh', 't ', 'so t', 'ha', 't i', 't il', 'lu', 'mi', 'na', 'te', 'd on', 'ly t', 'he ', 'ba', 'by’', 's ', 'form', ',', ' ', 'lea', 'vin', 'g t', 'he ', 'res', 't o', 'f t', 'he ', 'be', 'droo', 'm i', 'n ', 'dar', 'kness', '.', ' ', 'Lea', 'nin', 'g i', 'n ', 'clo', 'ser', ',', ' ', 'he ', 'si', 'len', 'tly ', 'mout', 'he', 'd t', 'he ', 'wor', 'ds ', 'Fas', 't a', 'sleep', '.', ' A', 's Y', 'o', 'ko’', 's ', 'pre', 'gnan', 'cy ', 'ha', 'd ', 'pro', 'gres', 'se', 'd an', 'd t', 'he ', 'fac', 't t', 'ha', 't ', 'he ', 'wa', 's ac', 'tual', 'ly ', 'goin', 'g ', 'to ', 'be ', 'a', ' ', 'fat', 'he', 'r ', 'be', 'ga', 'n ', 'to ', 'sin', 'k in', ',', ' ', 'he’', 'd ', 'wor', 'rie', 'd t', 'ha', 't t', 'he ', 'ba', 'by ', 'migh', 't ', 'ha', 've ', 'dif', 'fi', 'cul', 'ty ', 'slee', 'ping', '.', ' ', 'Ka', 'was', 'hi', 'ma ', 'ha', 'd ', 'suf', 'fe', 're', 'd ', 'fro', 'm in', 'som', 'nia', ' ', 'sin', 'ce ', 'e', 'le', 'men', 'ta', 'ry sc', 'hool', ',', ' and', ',', ' af', 'te', 'r all', ',', ' ', 'hi', 's ', 'bloo', 'd ', 'woul', 'd ', 'ru', 'n i', 'n t', 'hi', 's c', 'hil', 'd’', 's ', 'veins', '.', ' ', 'He’', 'd ', 'hear', 'd i', 't ', 'wa', 's ', 'nor', 'ma', 'l ', 'fo', 'r ', 'new', 'born', 's ', 'to ', 'slee', 'p ', 'vir', 'tual', 'ly ', 'a', 'roun', 'd t', 'he ', 'clock', ';', ' i', 'n ', 'fact', ',', ' ', 'he ', 'see', 'me', 'd ', 'to ', 're', 'cal', 'l ', 'so', 'me c', 'hild', '-', 'rea', 'rin', 'g ex', 'per', 't ', 'des', 'cri', 'bin', 'g ', 'slee', 'p a', 's a', 'n in', 'fan', 't’', 's ‘', 'jo', 'b’', '.', ' W', 'ha', 't ', 'coul', 'd ', 'be ', 'mo', 're ', 'tra', 'gic', ',', ' t', 'hen', ',', ' t', 'ha', 'n a', ' ', 'ba', 'by ', 'in', 'som', 'niac', '?', '\\n\\n', 'He ', 'tur', 'ne', 'd ', 'sof', 'tly ', 'to c', 'hec', 'k o', 'n Y', 'o', 'ko ', 'i', 'n t', 'he ', 'dou', 'ble ', 'be', 'd ', 'be', 'hin', 'd ', 'him', '.', ' ', 'He', 'r ', 're', 'gu', 'la', 'r ', 'breat', 'hin', 'g as', 'su', 're', 'd ', 'hi', 'm s', 'he ', 'wa', 's s', 'til', 'l a', 'sleep', '.', '\\n\\n', 'Ka', 'was', 'hi', 'ma ', 'ha', 'd ', 'bee', 'n ', 'doin', 'g t', 'hi', 's e', 've', 'ry ', 'nigh', 't ', 'la', 'te', 'ly', ',', ' s', 'tan', 'din', 'g t', 'he', 're ', 'ga', 'zin', 'g ', 'dow', 'n a', 't t', 'he ', 'ba', 'by w', 'hi', 'le ', 'hi', 's ', 'wi', 'fe ', 'slept', '.', ' ', 'Te', 'n ', 'nigh', 'ts i', 'n a', ' ', 'ro', 'w ', 'now', ',', ' ', 'to ', 'be ', 'e', 'xact', '.', ' I', 't ', 'wa', 's ', 'wel', 'l af', 'te', 'r ', 'mi', 'dnight', ',', ' an', 'd ', 'sin', 'ce Y', 'o', 'ko ', 'ro', 'se ', 'e', 'ar', 'ly ', 'e', 'ac', 'h ', 'mor', 'nin', 'g ', 'to ', 'pre', 'pa', 're ', 'fo', 'r ', 'work', ',', ' s', 'he ', 'wa', 'sn’', 't ', 'li', 'ke', 'ly ', 'to ', 'a', 'wa', 'ken', '.', ' A', ' w', 'ho', 'le', 'so', 'me ', 'an', 'd ', 'healt', 'hy ', 'twen', 'ty', '-', 'ni', 'ne', '-', 'ye', 'ar', '-', 'ol', 'd ', 'coo', 'kin', 'g ex', 'pert', ',', ' Y', 'o', 'ko ', 'wa', 's a', ' s', 'tran', 'ge', 'r ', 'to t', 'hin', 'gs ', 'li', 'ke ', 'in', 'som', 'nia', '.', ' S', 'he’', 'd ', 'qui', 't ', 'he', 'r ', 'jo', 'b ', 'wit', 'h a', ' ', 'ma', 'jo', 'r ', 'ma', 'nu', 'fac', 'tu', 're', 'r o', 'f ', 'ba', 'ke', 'd ', 'goo', 'ds w', 'he', 'n t', 'hey', ' ', 'mar', 'rie', 'd an', 'd ', 'be', 'gu', 'n ', 'gi', 'vin', 'g ', 'les', 'son', 's ', 'to ', 'peo', 'ple ', 'fro', 'm t', 'he ', 'neigh', 'bour', 'hood', ',', ' ', 'righ', 't ', 'he', 're ', 'i', 'n t', 'hei', 'r o', 'ne', '-', 'be', 'droo', 'm a', 'par', 'tment', '.', ' Y', 'o', 'ko’', 's ', 'brea', 'd an', 'd ', 'pas', 'try ', 'clas', 'se', 's ', 'pro', 've', 'd as', 'to', 'nis', 'hin', 'gly ', 'po', 'pu', 'lar', ',', ' an', 'd ', 'no', 'w s', 'he ', 'ha', 'd ', 'do', 'zen', 's o', 'f s', 'tu', 'den', 'ts —', ' ', 'fro', 'm ', 'hou', 'se', 'wi', 've', 's an', 'd ', 'mid', 'dle', '-sc', 'hoo', 'l ', 'girl', 's ', 'to ', 'el', 'der', 'ly ', 'wi', 'do', 'wer', 's an', 'd e', 've', 'n ', 'mid', 'dle', '-', 'a', 'ge', 'd ', 'men', '.', ' S', 'he ', 'taugh', 't ', 'clas', 'se', 's al', 'mos', 't e', 've', 'ry ', 'day', ',', ' ', 'ta', 'kin', 'g on', 'ly ', 'two ', 'fi', 'xe', 'd ', 'ho', 'li', 'day', 's a', ' ', 'month', ',', ' an', 'd t', 'he ', 'en', 'ti', 're ', 'a', 'par', 'tment', ',', ' in', 'clu', 'din', 'g t', 'hi', 's ', 'be', 'droom', ',', ' ', 'wa', 's ', 'per', 'mea', 'te', 'd ', 'wit', 'h t', 'he ', 'but', 'te', 'ry ', 'smel', 'l t', 'ha', 't ', 'fo', 'r ', 'Ka', 'was', 'hi', 'ma ', 'ha', 'd ', 'co', 'me ', 'to ', 'sym', 'bo', 'li', 'se ', 'hap', 'pi', 'ness', '.', ' ', 'Lit', 'tle ', 'Rie', ' ', '(', 'the ', 'na', 'me ', 'sug', 'ges', 'te', 'd ', 'by Y', 'o', 'ko’', 's ', 'mot', 'her', ')', ' ', 'wa', 's ', 'no', 'w ', 'fou', 'r ', 'mont', 'hs old', ',', ' an', 'd Y', 'o', 'ko ', 'so', 'me', 'ho', 'w ', 'ma', 'na', 'ge', 'd ', 'to ', 'loo', 'k af', 'te', 'r ', 'he', 'r an', 'd s', 'til', 'l ', 'main', 'tai', 'n a', ' ', 'ful', 'l ', 'teac', 'hin', 'g sc', 'he', 'du', 'le', '.', ' O', 'f ', 'cour', 'se', ',', ' i', 't ', 'di', 'dn’', 't ', 'hur', 't t', 'ha', 't ', 'mos', 't o', 'f ', 'he', 'r s', 'tu', 'den', 'ts ', 'we', 're ', 'fe', 'ma', 'le ', 'an', 'd al', 'way', 's e', 'a', 'ge', 'r ', 'to ', 'hel', 'p o', 'u', 't ', 'wit', 'h t', 'he ', 'ba', 'by.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = SyllableTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TabTokenizer\n",
    "\n",
    "Аналогичен SpaceTokenizer, только для разделения строки использует символ табуляции, что совпадает с применением метода s.split(‘\\t’)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu Murakami\\n\\nPIERCING\\n\\n\\n1\\n\\n\\n\\nA SMALL LIVING CREATURE asleep in its crib. Like a laboratory animal in a cage, thought Kawashima Masayuki. He used the palm of his hand to shade the penlight so that it illuminated only the baby’s form, leaving the rest of the bedroom in darkness. Leaning in closer, he silently mouthed the words Fast asleep. As Yoko’s pregnancy had progressed and the fact that he was actually going to be a father began to sink in, he’d worried that the baby might have difficulty sleeping. Kawashima had suffered from insomnia since elementary school, and, after all, his blood would run in this child’s veins. He’d heard it was normal for newborns to sleep virtually around the clock; in fact, he seemed to recall some child-rearing expert describing sleep as an infant’s ‘job’. What could be more tragic, then, than a baby insomniac?\\n\\nHe turned softly to check on Yoko in the double bed behind him. Her regular breathing assured him she was still asleep.\\n\\nKawashima had been doing this every night lately, standing there gazing down at the baby while his wife slept. Ten nights in a row now, to be exact. It was well after midnight, and since Yoko rose early each morning to prepare for work, she wasn’t likely to awaken. A wholesome and healthy twenty-nine-year-old cooking expert, Yoko was a stranger to things like insomnia. She’d quit her job with a major manufacturer of baked goods when they married and begun giving lessons to people from the neighbourhood, right here in their one-bedroom apartment. Yoko’s bread and pastry classes proved astonishingly popular, and now she had dozens of students — from housewives and middle-school girls to elderly widowers and even middle-aged men. She taught classes almost every day, taking only two fixed holidays a month, and the entire apartment, including this bedroom, was permeated with the buttery smell that for Kawashima had come to symbolise happiness. Little Rie (the name suggested by Yoko’s mother) was now four months old, and Yoko somehow managed to look after her and still maintain a full teaching schedule. Of course, it didn’t hurt that most of her students were female and always eager to help out with the baby.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = TabTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TextTilingTokenizer\n",
    "\n",
    "TextTiling – это метод автоматического разделения полноразмерных текстовых документов на последовательные блоки, состоящие из нескольких абзацев, которые представляют собой отрывки или подтемы. Алгоритм предполагает, что в ходе описания подтемы используется определенный набор слов, и когда подтема изменяется, значительная часть словарного запаса также изменяется."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "#     text = file.read()\n",
    "# tokenizer = TextTilingTokenizer(w= 8, k=10,stopwords='animal')\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# print(tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ToktokTokenizer\n",
    "\n",
    "Токенизатор Toktok – это простой универсальный токенизатор, на вход которого подается по одному предложению в строке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', ',', 'thought', 'Kawashima', 'Masayuki.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby', '’', 's', 'form', ',', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'Leaning', 'in', 'closer', ',', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep.', 'As', 'Yoko', '’', 's', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', ',', 'he', '’', 'd', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', ',', 'and', ',', 'after', 'all', ',', 'his', 'blood', 'would', 'run', 'in', 'this', 'child', '’', 's', 'veins.', 'He', '’', 'd', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock', ';', 'in', 'fact', ',', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant', '’', 's', '‘job', '’', '.', 'What', 'could', 'be', 'more', 'tragic', ',', 'then', ',', 'than', 'a', 'baby', 'insomniac', '?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', ',', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'Ten', 'nights', 'in', 'a', 'row', 'now', ',', 'to', 'be', 'exact.', 'It', 'was', 'well', 'after', 'midnight', ',', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', ',', 'she', 'wasn', '’', 't', 'likely', 'to', 'awaken.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert', ',', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'She', '’', 'd', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', ',', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'Yoko', '’', 's', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', ',', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'She', 'taught', 'classes', 'almost', 'every', 'day', ',', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', ',', 'and', 'the', 'entire', 'apartment', ',', 'including', 'this', 'bedroom', ',', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'Little', 'Rie', '(', 'the', 'name', 'suggested', 'by', 'Yoko', '’', 's', 'mother', ')', 'was', 'now', 'four', 'months', 'old', ',', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule.', 'Of', 'course', ',', 'it', 'didn', '’', 't', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TweetTokenizer\n",
    "\n",
    "Это токенизатор с поддержкой особенностей коротких сообщений Twitter, разработанный для гибкой и простой адаптации к новым предметным областям и задачам. Основная логика его работы заключается в следующем:\n",
    "\n",
    "Определяется список регулярных выражений, для поиска в исходном тексте гиперссылок, смайликов, телефонных номеров, имен пользователей Twitter, хештегов и др.\n",
    "Регулярные выражения помещаются по порядку в скомпилированный объект регулярного выражения, называемый word_re.\n",
    "Выполняется токенизация с помощью метода word_re.findall(s), где s – строка, исходного текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib', '.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', ',', 'thought', 'Kawashima', 'Masayuki', '.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby', '’', 's', 'form', ',', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness', '.', 'Leaning', 'in', 'closer', ',', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep', '.', 'As', 'Yoko', '’', 's', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', ',', 'he', '’', 'd', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping', '.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', ',', 'and', ',', 'after', 'all', ',', 'his', 'blood', 'would', 'run', 'in', 'this', 'child', '’', 's', 'veins', '.', 'He', '’', 'd', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock', ';', 'in', 'fact', ',', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant', '’', 's', '‘', 'job', '’', '.', 'What', 'could', 'be', 'more', 'tragic', ',', 'then', ',', 'than', 'a', 'baby', 'insomniac', '?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him', '.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep', '.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', ',', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept', '.', 'Ten', 'nights', 'in', 'a', 'row', 'now', ',', 'to', 'be', 'exact', '.', 'It', 'was', 'well', 'after', 'midnight', ',', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', ',', 'she', 'wasn', '’', 't', 'likely', 'to', 'awaken', '.', 'A', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert', ',', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia', '.', 'She', '’', 'd', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', ',', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment', '.', 'Yoko', '’', 's', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', ',', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men', '.', 'She', 'taught', 'classes', 'almost', 'every', 'day', ',', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', ',', 'and', 'the', 'entire', 'apartment', ',', 'including', 'this', 'bedroom', ',', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness', '.', 'Little', 'Rie', '(', 'the', 'name', 'suggested', 'by', 'Yoko', '’', 's', 'mother', ')', 'was', 'now', 'four', 'months', 'old', ',', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule', '.', 'Of', 'course', ',', 'it', 'didn', '’', 't', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = TweetTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WordPunctTokenizer\n",
    "\n",
    "Преобразует текст в последовательность буквенных и неалфавитных символов с помощью регулярного выражения ‘\\w+|[^\\w\\s]+’. Отличается от WhitespaceTokenizer тем, что отделяет знаки препинания и иные символы пунктуации от слова, за / перед которым они идут, и возвращает их в качестве отдельных токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ryu', 'Murakami', 'PIERCING', '1', 'A', 'SMALL', 'LIVING', 'CREATURE', 'asleep', 'in', 'its', 'crib', '.', 'Like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', ',', 'thought', 'Kawashima', 'Masayuki', '.', 'He', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby', '’', 's', 'form', ',', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness', '.', 'Leaning', 'in', 'closer', ',', 'he', 'silently', 'mouthed', 'the', 'words', 'Fast', 'asleep', '.', 'As', 'Yoko', '’', 's', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', ',', 'he', '’', 'd', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping', '.', 'Kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', ',', 'and', ',', 'after', 'all', ',', 'his', 'blood', 'would', 'run', 'in', 'this', 'child', '’', 's', 'veins', '.', 'He', '’', 'd', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock', ';', 'in', 'fact', ',', 'he', 'seemed', 'to', 'recall', 'some', 'child', '-', 'rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant', '’', 's', '‘', 'job', '’.', 'What', 'could', 'be', 'more', 'tragic', ',', 'then', ',', 'than', 'a', 'baby', 'insomniac', '?', 'He', 'turned', 'softly', 'to', 'check', 'on', 'Yoko', 'in', 'the', 'double', 'bed', 'behind', 'him', '.', 'Her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep', '.', 'Kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', ',', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept', '.', 'Ten', 'nights', 'in', 'a', 'row', 'now', ',', 'to', 'be', 'exact', '.', 'It', 'was', 'well', 'after', 'midnight', ',', 'and', 'since', 'Yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', ',', 'she', 'wasn', '’', 't', 'likely', 'to', 'awaken', '.', 'A', 'wholesome', 'and', 'healthy', 'twenty', '-', 'nine', '-', 'year', '-', 'old', 'cooking', 'expert', ',', 'Yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia', '.', 'She', '’', 'd', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', ',', 'right', 'here', 'in', 'their', 'one', '-', 'bedroom', 'apartment', '.', 'Yoko', '’', 's', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', ',', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle', '-', 'school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle', '-', 'aged', 'men', '.', 'She', 'taught', 'classes', 'almost', 'every', 'day', ',', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', ',', 'and', 'the', 'entire', 'apartment', ',', 'including', 'this', 'bedroom', ',', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'Kawashima', 'had', 'come', 'to', 'symbolise', 'happiness', '.', 'Little', 'Rie', '(', 'the', 'name', 'suggested', 'by', 'Yoko', '’', 's', 'mother', ')', 'was', 'now', 'four', 'months', 'old', ',', 'and', 'Yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule', '.', 'Of', 'course', ',', 'it', 'didn', '’', 't', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby', '.']\n"
     ]
    }
   ],
   "source": [
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токенизатор с использованием алгоритма REPP, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "#     text = file.read()\n",
    "# tokenizer = ReppTokenizer(repp_dir=\"eng_text.txt\")\n",
    "# tokens = tokenizer.tokenize(text)\n",
    "\n",
    "# print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0.6881720430107527}\n"
     ]
    }
   ],
   "source": [
    "def lexical_diversity(book_path, encoding=\"utf-8\"):\n",
    "    with open(book_path, 'r', encoding=encoding, errors='ignore') as file:\n",
    "        text = file.read()\n",
    "\n",
    "    tokens = text.split()\n",
    "   \n",
    "    unique_words = set(tokens)\n",
    "    num_unique_words = len(unique_words)\n",
    "    \n",
    "    lexical_diversity = num_unique_words / len(tokens)\n",
    "\n",
    "    return lexical_diversity\n",
    "\n",
    "book_path = \"eng_text.txt\" \n",
    "diversity_score = lexical_diversity(book_path)\n",
    "print({diversity_score})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ryu', 'murakami', 'piercing', '1', 'a', 'small', 'living', 'creature', 'asleep', 'in', 'its', 'crib.', 'like', 'a', 'laboratory', 'animal', 'in', 'a', 'cage', 'thought', 'kawashima', 'masayuki.', 'he', 'used', 'the', 'palm', 'of', 'his', 'hand', 'to', 'shade', 'the', 'penlight', 'so', 'that', 'it', 'illuminated', 'only', 'the', 'baby’s', 'form', 'leaving', 'the', 'rest', 'of', 'the', 'bedroom', 'in', 'darkness.', 'leaning', 'in', 'closer', 'he', 'silently', 'mouthed', 'the', 'words', 'fast', 'asleep.', 'as', 'yoko’s', 'pregnancy', 'had', 'progressed', 'and', 'the', 'fact', 'that', 'he', 'was', 'actually', 'going', 'to', 'be', 'a', 'father', 'began', 'to', 'sink', 'in', 'he’d', 'worried', 'that', 'the', 'baby', 'might', 'have', 'difficulty', 'sleeping.', 'kawashima', 'had', 'suffered', 'from', 'insomnia', 'since', 'elementary', 'school', 'and', 'after', 'all', 'his', 'blood', 'would', 'run', 'in', 'this', 'child’s', 'veins.', 'he’d', 'heard', 'it', 'was', 'normal', 'for', 'newborns', 'to', 'sleep', 'virtually', 'around', 'the', 'clock;', 'in', 'fact', 'he', 'seemed', 'to', 'recall', 'some', 'child-rearing', 'expert', 'describing', 'sleep', 'as', 'an', 'infant’s', '‘job’.', 'what', 'could', 'be', 'more', 'tragic', 'then', 'than', 'a', 'baby', 'insomniac?', 'he', 'turned', 'softly', 'to', 'check', 'on', 'yoko', 'in', 'the', 'double', 'bed', 'behind', 'him.', 'her', 'regular', 'breathing', 'assured', 'him', 'she', 'was', 'still', 'asleep.', 'kawashima', 'had', 'been', 'doing', 'this', 'every', 'night', 'lately', 'standing', 'there', 'gazing', 'down', 'at', 'the', 'baby', 'while', 'his', 'wife', 'slept.', 'ten', 'nights', 'in', 'a', 'row', 'now', 'to', 'be', 'exact.', 'it', 'was', 'well', 'after', 'midnight', 'and', 'since', 'yoko', 'rose', 'early', 'each', 'morning', 'to', 'prepare', 'for', 'work', 'she', 'wasn’t', 'likely', 'to', 'awaken.', 'a', 'wholesome', 'and', 'healthy', 'twenty-nine-year-old', 'cooking', 'expert', 'yoko', 'was', 'a', 'stranger', 'to', 'things', 'like', 'insomnia.', 'she’d', 'quit', 'her', 'job', 'with', 'a', 'major', 'manufacturer', 'of', 'baked', 'goods', 'when', 'they', 'married', 'and', 'begun', 'giving', 'lessons', 'to', 'people', 'from', 'the', 'neighbourhood', 'right', 'here', 'in', 'their', 'one-bedroom', 'apartment.', 'yoko’s', 'bread', 'and', 'pastry', 'classes', 'proved', 'astonishingly', 'popular', 'and', 'now', 'she', 'had', 'dozens', 'of', 'students', '—', 'from', 'housewives', 'and', 'middle-school', 'girls', 'to', 'elderly', 'widowers', 'and', 'even', 'middle-aged', 'men.', 'she', 'taught', 'classes', 'almost', 'every', 'day', 'taking', 'only', 'two', 'fixed', 'holidays', 'a', 'month', 'and', 'the', 'entire', 'apartment', 'including', 'this', 'bedroom', 'was', 'permeated', 'with', 'the', 'buttery', 'smell', 'that', 'for', 'kawashima', 'had', 'come', 'to', 'symbolise', 'happiness.', 'little', 'rie', '(the', 'name', 'suggested', 'by', 'yoko’s', 'mother)', 'was', 'now', 'four', 'months', 'old', 'and', 'yoko', 'somehow', 'managed', 'to', 'look', 'after', 'her', 'and', 'still', 'maintain', 'a', 'full', 'teaching', 'schedule.', 'of', 'course', 'it', 'didn’t', 'hurt', 'that', 'most', 'of', 'her', 'students', 'were', 'female', 'and', 'always', 'eager', 'to', 'help', 'out', 'with', 'the', 'baby.']\n"
     ]
    }
   ],
   "source": [
    "class Tokenizer:\n",
    "    def tokenizer(self, text):\n",
    "        text = text.lower()\n",
    "        text = text.replace(',', ' ')\n",
    "        tokens = text.split()\n",
    "        return tokens\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "with open(\"eng_text.txt\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "    tokens = tokenizer.tokenizer(text)\n",
    "\n",
    "print(tokens)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
